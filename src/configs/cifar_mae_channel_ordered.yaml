defaults:
  - lrscheduler: "Constant"

gpu_idx: 0
seed: 42
nEpochs: 200

outputpath: "/home/user/results/debug_cifar_mae"
experimentname: "channel_order"
resume: False
resumeCheckpoint: ""

trainroutine:
  _target_: "train.CIFAR_MAE" # its not yet implementd rather hardcoded
  _partial_: true

loss:
  _target_: "torch.nn.MSELoss"

model:
  _target_: "model.MAE_ViT"
  encoder:
    _target_: "model.MAE_Encoder_ChannelWise"
    image_size: 256
    num_channels: 3
    patch_size: 16
    emb_dim: 400
    num_layer: 8
    num_head: 8
    mask_ratio: 100 # % of the patches that are shown to the model should be between 0 and 100
    keep_one_channel_per_patch: True
  decoder:
    _target_: "model.MAE_Decoder_ChannelWise"
    image_size: 256
    num_channels: 3
    patch_size: 16
    emb_dim: 400
    num_layer: 3
    num_head: 8

optimizer:
  _target_: "torch.optim.Adam"
  lr: 1e-4
  base_learning_rate: 1e-4
  weight_decay: 0.05

scheduler: 
  warmup_epoch: 3

dataloader: 
  _target_: "dataloader.CIFAR100"
  _partial_: true
  topdir_dataset: "/home/user/data/cifar100"
  relative_locations_file: ""
  resample_img: True
  resample_img_size: 256
  shuffle: True
  batch_size: 64
  threads: 16
  drop_last_batch: False

restrict_train_data: -1
restrict_val_data: -1
validation_every_N_sampels: -1
validate_after_every_n_epoch: -1
plotting_every_N_sampels: 100000
special_save_nEpoch: [5,10,20,30,40,50]